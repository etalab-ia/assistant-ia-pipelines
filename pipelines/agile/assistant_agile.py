"""
title: Albert Rag
author: Camille Andre
version: 0.1
Description: Un assistant qui r√©pond √† des questions sur les d√©marches administratives, peut proposer d'utiliser internet via Albert API si n√©cessaire.
"""

import re
import requests
from typing import List, Optional, Dict
from openai import OpenAI
from bs4 import BeautifulSoup
from pydantic import BaseModel, Field
from typing import Literal
import logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("mon_logger")
logger.setLevel(logging.DEBUG)

PIPELINE_NAME = "Assistant Agile"
collection_dict = {"Assistant_Agile": 2657}

#### PROMPTS ####
## SYSTEM PROMPT ##
SYSTEM_PROMPT = """
[R√¥le]
Tu es Assistant Agile, un accompagnateur bienveillant qui aide des agents publics et leurs √©quipes √† mettre en pratique l‚Äôagilit√© au quotidien (√©coute, coop√©ration, it√©ration, orientation impact).

[Langue]
R√©ponds en fran√ßais simple et positif.

[Ce que tu peux faire]
- Poser d‚Äôabord 2 √† 3 questions de clarification contextuelles, si c'est pertinent.
- Fournir des conseils concrets, imm√©diatement actionnables, adapt√©s au secteur public (comit√©s, indicateurs d‚Äôimpact, budgets, parties prenantes).
- Proposer des trames, canevas ou rituels agiles (r√©tros, check-in, fiche produit, vision, priorisation‚Ä¶) quand c'est pertinent, pas a chaque fois.
- Ancrer les apprentissages par de petites √©tapes, limit√©es dans le temps, avec crit√®res de succ√®s.

[Ce que tu ne fais pas]
- Pas de propos d√©sobligeants, d‚Äôinsultes, de racisme, ni de discrimination.
- Pas de conseils psychologiques ou personnels.
- Pas de r√©ponses techniques pointues (pas de code, pas de dev).
- Tu n‚Äôes ni manager ni formateur acad√©mique : tu facilites.

[Utilisation du contexte (RAG)]
Tu peux t‚Äôinspirer des informations de contexte donn√©e dans le message utilisateur, extraites de documents de l‚Äôorganisation. Si elles existent, privil√©gie-les; si une info est absente, n‚Äôinvente pas.

R√®gles d‚Äôusage du contexte:
- Si une info du contexte r√©pond directement √† la question, utilise-la mais ne parle jamais "du contexte" dans ta r√©ponse.
- S‚Äôil y a ambigu√Øt√© ou contradictions, signale-le bri√®vement et pose des questions pour clarifier.
- Ne cite que l‚Äôessentiel (1‚Äì3 points) du contexte; pas de copie massive.

Ce que tu peux mettre dans ta r√©ponse mais pas obligatoirement :
- Questions de clarification (2‚Äì3, concises), si c'est pertinent.
- Proposition simple et actionnable (3‚Äì6 √©tapes max) adapt√©e aux r√©ponses probables.
- Outil/Canevas si utile (trame pr√™te √† l‚Äôemploi), si c'est pertinent.
- Prochaine micro‚Äë√©tape et mesure de succ√®s.

[Ton]
Chaleureux, pragmatique, orient√© impact et apprentissage. Utilise des puces, des titres courts, et √©vite le jargon inutile.

[Adaptation secteur public]
- Sensibilit√© aux processus de d√©cision (jury, comit√© produit, arbitrages, financement).
- Mettre l‚Äôaccent sur les indicateurs d‚Äôimpact et la valeur publique.
- Encourager l‚Äôit√©ration courte et la transparence des d√©cisions.

[Gestion de l‚Äôincertitude]
Si des √©l√©ments cl√©s manquent (ex. participants, dur√©e, objectif), commence par poser des questions √† l'utilisateur pour clarifier, puis propose un plan ‚Äúpar d√©faut‚Äù modulable.

[Consignes finales]
- Pas de termes culpabilisants; valorise les progr√®s incr√©mentaux.
- Si le contexte est vide r√©ponds de mani√®re g√©n√©rique et demande les infos manquantes.
- Fini toujours tes r√©ponses par une question √† l'utilisateur pour encourager la discussion.
"""

#[Exemples compacts]
#Utilisateur: ‚ÄúDemain j‚Äôanime une r√©tro et je ne sais pas comment la structurer.‚Äù
#R√©ponse (extrait):
#Questions: ‚ÄúCombien de temps as-tu ? Combien de participants ? L‚Äô√©quipe a-t-elle l‚Äôhabitude des r√©tros ?‚Äù
#- Trame: ‚Äú5 min check-in ‚Üí 10 min faits marquants ‚Üí 10 min ce qui aide/ce qui bloque ‚Üí 10 min id√©es ‚Üí 10 min plan d‚Äôaction (1‚Äì2 engagements, responsables, √©ch√©ance).‚Äù
#- Outil: ‚ÄúIcebreaker m√©t√©o projet (soleil/nuage/pluie).‚Äù
#- Prochaine √©tape: ‚ÄúPlanifie le suivi dans 2 semaines avec 1 indicateur simple (ex. d√©lai de traitement des demandes).‚Äù
#- Tu veux que je t‚Äôenvoie une fiche minute pour l‚Äôanimer demain ? <- Question de relance pour encourager l‚Äôexp√©rimentation.


## DEFAULT PROMPT ##
PROMPT = """
<informations de contexte trouv√©es>
{context}
</informations de contexte trouv√©es>

En t'aidant si besoin du le contexte ci-dessus et de la conversation, r√©ponds au message :

{question}
"""

## CONTEXT FOR NO CONTEXT QUESTION ##
NO_CONTEXT_MEMORY = """Aucun contexte n'est n√©cessaire, r√©ponds gentillement √† l'utilisateur. N'ajoutes pas de sources en fin de r√©ponse. Si besoin, voil√† les informations que tu connais ; 
- Tu es un mod√®le opensource adapt√© par Etalab pour aider les agents publics, sp√©cialis√© en management et m√©thodes agiles.
- Tu es connect√© a des bases de donn√©es sp√©cialis√©es en management et m√©thodes agiles.
- L'utilisateur peut regarder en haut a gauche s'il trouve un autre agent sp√©cialis√© dans la liste qui pourrait l'aider pour sa question si elle n'est pas sur ton sujet a toi.
"""

## PROMPT FOR CONTEXTUALISED SEARCH ##
PROMPT_SEARCH = """
Tu es un assistant qui cherche des documents dans une base de donn√©es sp√©cialis√©e en management et m√©thodes agiles pour r√©pondre √† une question.
Exemples pour t'aider: 
<history>
Comment organiser une r√©trospective efficace ?
</history>
r√©ponse attendue : 
r√©trospective agile organisation efficace

<history>
Coucou
</history>
r√©ponse attendue : 
no_search

<history>
Quelles sont les bonnes pratiques pour un daily standup ?
</history>
r√©ponse attendue : 
daily standup bonnes pratiques agile

<history>
Comment d√©finir des OKR dans mon √©quipe ?
</history>
r√©ponse attendue : 
OKR d√©finition √©quipe management

<history>
Tu sais faire quoi ?
</history>
r√©ponse attendue : 
no_search

<history>
Quel framework agile choisir pour mon projet ?
</history>
r√©ponse attendue : 
framework agile choix projet scrum kanban

En te basant sur cet historique de conversation : 
<history>
{history}
</history>
question de l'utilisateur : {question}
R√©ponds avec uniquement une recherche pour trouver des documents sur le management et l'agile qui peuvent t'aider √† r√©pondre √† la derni√®re question de l'utilisateur.
R√©ponds uniquement avec la recherche, rien d'autre, sous forme d'une question claire et pr√©cise.
Si l'utilisateur parle du mod√®le lui m√™me, r√©ponds no_search.
Si la question ne concerne pas le management, l'agile, les m√©thodes de travail ou l'organisation d'√©quipe, r√©ponds no_search.
Si aucune recherche n'est n√©cessaire, r√©ponds no_search.
"""

## Prompt for confidence compute ##
PROMPT_CONFIDENCE = """
Tu es un assistant qui √©value la confiance de la r√©ponse d'un assistant.
Voil√† un contexte :
{context}
Voil√† une question :
{question}
Voil√† une r√©ponse :
{answer}
R√©ponds avec une note entre 0 et 100 pour la confiance de la r√©ponse en se basant sur le contexte et la question pos√©e.
Si la r√©ponse est "Je ne sais pas" et ne contient pas de sources, r√©ponds 100.
R√©ponds uniquement avec une note entre 0 et 100 et aucun commentaire.
note : 
"""

def format_chunks_to_text(chunk: dict) -> str:
    """Format chunk metadata to human-readable text."""
    title = f"Document : {chunk['chunk']['metadata']['document_name']}"
    content = f"Content : {chunk['chunk']['content']}"
    return f"{title}\n{content}\n"


def confidence_message(
    client, model, context, question, answer
):
    """Calculate confidence score and yield status message if confidence is low."""
    try:
        confidence = client.chat.completions.create(
            model=model,
            stream=False,
            temperature=0.1,
            max_tokens=3,
            messages=[
                {
                    "role": "user",
                    "content": PROMPT_CONFIDENCE.format(
                        context=context, question=question, answer=answer
                    ),
                }
            ],
        )
        confidence_text = confidence.choices[0].message.content
        confidence_match = re.search(r"\b([0-9]|[1-9][0-9]|100)\b", confidence_text)
        confidence = int(confidence_match.group(1)) if confidence_match else 0
        logger.info(f"Confidence score calculated: {confidence}")
        
        # Only yield warning if confidence is below threshold
        if confidence < 80:
            yield {
                "event": {
                    "type": "status",
                    "data": {
                        "description": f"üî¥ Indice de confiance faible : {confidence}% ‚Äî Prenez cette r√©ponse avec pr√©caution.",
                        "done": True,
                        "hidden": False,
                    },
                }
            }
        else:
            return 0
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration de la note de confiance: {e}")
        return 0

def stream_albert(
    client,
    model,
    max_tokens,
    messages,
    response_collector=None
):
    """Stream Albert API response."""
    logger.info("=== STREAMING ALBERT RESPONSE ===")
    logger.info(f"Model: {model}")

    try:
        chat_response = client.chat.completions.create(
            model=model,
            stream=True,
            temperature=0.2,
            max_tokens=max_tokens,
            messages=messages,
        )

        output = ""
        word_buffer = ""
        for chunk in chat_response:
            try:
                # Check if chunk has valid structure
                choices = getattr(chunk, "choices", None)
                if not choices or not hasattr(choices[0], "delta"):
                    continue

                delta = choices[0].delta
                finish_reason = getattr(choices[0], "finish_reason", None)
                token = getattr(delta, "content", "") if delta else ""

                if token:
                    output += token
                    word_buffer += token
                    # Yield only when a space is detected (i.e., a word is complete)
                    while " " in word_buffer:
                        word, word_buffer = word_buffer.split(" ", 1)
                        # Add the space back to the word
                        word += " "
                        yield {
                            "event": {
                                "type": "message",
                                "data": {
                                    "content": word,
                                    "done": False,
                                },
                            }
                        }
                # At the end, after finish_reason, flush any remaining word_buffer
                if finish_reason is not None:
                    if word_buffer:
                        yield {
                            "event": {
                                "type": "message",
                                "data": {
                                    "content": word_buffer,
                                    "done": False,
                                },
                            }
                        }
                    break

            except Exception as inner_e:
                logger.error(f"Error in streaming chunk: {inner_e}")
                continue
                
                # Exit when generation is complete
                if finish_reason is not None:
                    break

            except Exception as inner_e:
                logger.error(f"Error in streaming chunk: {inner_e}")
                continue

        # Store complete response if collector provided
        if response_collector is not None:
            response_collector.append(output)

        # Signal completion
        yield {
            "event": {
                "type": "message",
                "data": {"content": "", "done": True},
            }
        }
        logger.info("Albert streaming completed successfully")

    except Exception as e:
        logger.error(f"Global API error: {e}")
        yield {
            "event": {
                "type": "chat:message:delta",
                "data": {
                    "content": f"Erreur globale API : {str(e)}",
                    "done": True,
                },
            }
        }

def search_api_albert(
    collection_ids: List[int],
    user_query: str,
    api_url: str,
    api_key: str = "",
    top_k: int = 5,
    rff_k: int = 20,
    method: str = "semantic",
    score_threshold: float = 0,
    web_search: bool = False,
) -> Optional[Dict]:
    """Performs search in Albert API collections."""
    if method == "hybrid":
        score_threshold = 0
    url = f"{api_url}/search"
    headers = {
        "accept": "application/json",
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }

    payload = {
        "collections": collection_ids,
        "rff_k": rff_k,
        "k": top_k,
        "method": method,
        "score_threshold": score_threshold,
        "web_search": web_search,
        "prompt": user_query,
        "additionalProp1": {},
    }

    try:
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        
        # Filter results by score threshold (manual verification)
        results = []
        for result in response.json().get("data", []):
            if result.get("score", 0) >= score_threshold:
                results.append(result)
        return results

    except requests.RequestException as e:
        logger.error(f"HTTP request error: {e}")
        raise
    except ValueError as e:
        logger.error(f"JSON parsing error: {e}")
        raise


def webpage_to_human_readable(page_content):
    """Convert HTML to readable text."""
    soup = BeautifulSoup(page_content, "html.parser")
    
    # Remove non-content elements
    for element in soup(
        ["script", "style", "meta", "link", "noscript", "header", "footer", "aside"]
    ):
        element.decompose()
    
    text = soup.get_text(separator="\n")
    # Clean up whitespace
    cleaned_text = "\n".join(line.strip() for line in text.splitlines() if line.strip())
    return cleaned_text


def reranker(
    query: str,
    chunks: list,
    api_url: str,
    api_key: str = "",
    score_threshold: Optional[float] = 0,
    rerank_model: str = "BAAI/bge-reranker-v2-m3",
    min_chunks: int = 1,
):
    """Reorder documents by relevance using Albert's reranking API."""
    chunks_questions = []

    chunks_questions.extend((chunk.get("chunk").get("content")) for chunk in chunks)

    request_body = {
        "prompt": query,
        "input": chunks_questions,
        "model": rerank_model,
    }

    headers = {"Authorization": f"Bearer {api_key}"}

    response = requests.post(
        url=f"{api_url}/rerank", json=request_body, headers=headers
    )

    if response.status_code == 200:
        results = response.json()
        if "data" in results:
            # Get indices sorted by highest score
            ranked_indices = [item["index"] for item in results["data"]]
            
            if score_threshold is not None:
                reranked_chunks = [
                    chunks[idx]
                    for idx in ranked_indices
                    if results["data"][idx]["score"] >= score_threshold
                ]
            else:
                reranked_chunks = [chunks[idx] for idx in ranked_indices]
            
            # Fallback to original chunks if too few results after filtering
            if len(reranked_chunks) < min_chunks:
                logger.warning("Rerank failed (score threshold), returning original chunks")
                return chunks
            else:
                logger.info("Rerank success, returning reranked chunks")
                return reranked_chunks
        else:
            logger.error(f"Format de r√©ponse inattendu: {results}")
            return chunks
    else:
        logger.error(f"Erreur lors du reranking: {response.status_code}")
        return chunks


def pipe_rag(
    self,
    body: dict,
    __event_emitter__=None,
    user_message=None,
    model_id=None,
    messages=None,
    collection_dict: dict = None,
    SYSTEM_PROMPT: str = None,
    PROMPT: str = None,
    format_chunks_to_text: callable = None,
):
    """Main RAG pipeline function."""
    prompt = body["messages"][-1]["content"]

    logger.info("=== PIPE RAG STARTED ===")
    
    # Load configuration
    ALBERT_API_URL = self.valves.ALBERT_API_URL
    ALBERT_API_KEY = self.valves.ALBERT_API_KEY 

    model = self.valves.MODEL
    rerank_model = self.valves.RERANK_MODEL
    number_of_chunks = self.valves.NUMBER_OF_CHUNKS
    number_of_chunks_reranker = self.valves.NUMBER_OF_CHUNKS_RERANKER
    max_tokens = 4096

    user_query = body.get("messages", [])[-1]["content"]

    client = OpenAI(
        api_key=ALBERT_API_KEY,
        base_url=ALBERT_API_URL,
    )
    logger.info("Configuration loaded successfully")
    
    # Prepare messages: System prompt + conversation history
    messages = []
    messages.append(
        {
            "role": "system",
            "content": SYSTEM_PROMPT,
        }
    )

    # Add conversation history with rolling window
    history = body.get("messages", [])
    max_hist = self.valves.max_turns
    if history and max_hist > 0:
        recent_history = history[-max_hist:]
        # Find first user message to ensure proper conversation flow
        start_idx = 0
        for i, msg in enumerate(recent_history):
            if msg.get("role") == "user":
                start_idx = i
                break
        messages += recent_history[start_idx:]

    # Search for relevant documents
    try:
        # Generate search query using LLM
        search = client.chat.completions.create(
            model=model,
            stream=False,
            temperature=0.1,
            max_tokens=50,
            messages=[
                {
                    "role": "user",
                    "content": PROMPT_SEARCH.format(
                        history=messages[1:],
                        question=prompt,
                    ),
                }
            ],
        )
        logger.debug(f"Collection dict: {collection_dict}")
        logger.debug(f"Search messages: {[msg for msg in messages[1:]]}")

        search = search.choices[0].message.content
        logger.info(f"Generated search query: {search}")
        
        if search.strip().lower() == "no_search":
            top_chunks = []
            context = NO_CONTEXT_MEMORY
        else:
            yield {
                "event": {
                    "type": "status",
                    "data": {
                        "description": f"Recherche en cours pour '{search}'",
                        "done": False,
                        "hidden": False,
                    },
                }
            }

            # Handle internet search vs collection search
            #if list(collection_dict.keys()) == ["internet"] or ("internet" in search.strip().lower() and PROPOSE_NET):
            #    logger.info("Performing internet search")
            #    context = search_internet(ALBERT_API_URL, ALBERT_API_KEY, search)
            #    context = ''
            #    logger.debug(f"Internet search context retrieved: \n\n\n{context}\n\n\n")
            #    context = f"Voila ce qui a √©t√© trouv√© sur internet : {context} R√©ponds √† l'utilisateur en utilisant ce qui a √©t√© trouv√©."
            #else:
            logger.info("Performing Albert API search")
            top_chunks = search_api_albert(
                collection_ids=list(collection_dict.values()),
                user_query=search,
                api_url=ALBERT_API_URL,
                api_key=ALBERT_API_KEY,
                top_k=number_of_chunks,
                rff_k=number_of_chunks,
                method=self.valves.SEARCH_METHOD,
                score_threshold=self.valves.SEARCH_SCORE_THRESHOLD,
                web_search=False,
            )
            logger.info(f"Search results found: {len(top_chunks)}")
            
            # Rerank results for better relevance
            #logger.info("Starting reranking process")
            if rerank_model != "None":
                top_chunks = reranker(
                    query=user_query,
                    chunks=top_chunks,
                    api_url=ALBERT_API_URL,
                    api_key=ALBERT_API_KEY,
                    score_threshold=self.valves.RERANKER_SCORE_THRESHOLD,
                    rerank_model=rerank_model,
                    min_chunks=number_of_chunks,
                )[:number_of_chunks_reranker]

            # Format context from chunks
            references = ""
            for k, chunk in enumerate(top_chunks):
                references += f"""
- Document {k+1}:
{format_chunks_to_text(chunk = chunk)}
"""
            logger.debug(f"Context generated: {len(references)} characters")
            context = references

    except Exception as e:
        logger.error(f"Error during search process: {e}")
        yield {
            "event": {
                "type": "status",
                "data": {
                    "description": "Erreur lors de la recherche.",
                    "done": True,
                    "hidden": False,
                },
            }
        }
        return "D√©sol√©, on dirait que la connection √† AlbertAPI est perdue. Veuillez r√©essayer plus tard."

    yield {
        "event": {
            "type": "status",
            "data": {
                "description": "Termin√©.",
                "done": True,
                "hidden": True,
            },
        }
    }

    # Add user query with context to messages
    messages[-1] = {
            "role": "user",
            "content": PROMPT.format(context=context, question=prompt),
        }

    logger.debug("Messages prepared for Albert response generation")
    [logger.info(f"Message: {str(msg)[:50]}...") for msg in messages]

    # Generate and stream response
    answer = ""
    for resp in stream_albert(client, model, max_tokens, messages, __event_emitter__):
        answer += resp["event"]["data"]["content"]
        yield resp

    logger.info(f"Final answer generated: {answer[:100]}...")
    
    # Calculate and display confidence if needed
    for conf_event in confidence_message(client, model, context, prompt, answer):
        yield conf_event

    # Add assistant response to conversation
    body["messages"].append(
        {
            "role": "assistant",
            "content": answer,
        }
    )
    return body


class Pipeline:
    class Valves(BaseModel):
        max_turns: int = Field(
            default=5, description="Maximum conversation turns taken into account for a user."
        )

        ALBERT_API_URL: str = Field(default="https://albert.api.etalab.gouv.fr/v1")
        ALBERT_API_KEY: str = Field(default="")
        MODEL: Literal[
            "albert-small",
            "albert-large",
        ] = Field(default="albert-large")
        RERANK_MODEL: Literal["BAAI/bge-reranker-v2-m3", "None"] = Field(default="BAAI/bge-reranker-v2-m3")
        NUMBER_OF_CHUNKS: int = Field(default=20)
        NUMBER_OF_CHUNKS_RERANKER: int = Field(default=10)
        SEARCH_SCORE_THRESHOLD: float = Field(default=0, description="Score threshold for the search API. 0 if method is hybrid.")
        SEARCH_METHOD: Literal["semantic", "hybrid"] = Field(default="hybrid", description="Method for the search API. semantic if method is hybrid.")
        RERANKER_SCORE_THRESHOLD: float = Field(default=0.1)
        pass
 
    def __init__(self):
        self.name = PIPELINE_NAME
        self.valves = self.Valves()
        self.collection_dict = collection_dict
        self.SYSTEM_PROMPT = SYSTEM_PROMPT
        self.PROMPT = PROMPT
        if self.valves.SEARCH_METHOD == "hybrid":
            self.valves.SEARCH_SCORE_THRESHOLD = 0

    async def on_startup(self):
        """Called when the server is started."""
        logger.info(f"Pipeline startup: {__name__}")

    async def on_shutdown(self):
        """Called when the server is stopped."""
        logger.info(f"Pipeline shutdown: {__name__}")

    def pipe(
        self,
        body: dict,
        __event_emitter__=None,
        user_message=None,
        model_id=None,
        messages=None,
    ):
        """Main pipeline entry point."""
        return pipe_rag(
            self,
            body,
            __event_emitter__,
            user_message,
            model_id,
            messages,
            self.collection_dict,
            self.SYSTEM_PROMPT,
            self.PROMPT,
            format_chunks_to_text,
        )
